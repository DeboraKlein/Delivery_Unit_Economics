{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6600fec0-c81a-4487-bba2-5a3f5c14549b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook: 02_Final_Quality_Checks.ipynb\n",
    "\n",
    "processo_nome = 'final_data_testing'\n",
    "status_final = 'sucesso'\n",
    "detalhes_log = 'Todos os testes de integridade concluídos.'\n",
    "violations_ref = 0\n",
    "violations_const = 0\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO pl_delivery_analysis.log_processamento (processo, etapa, status, timestamp, detalhes)\n",
    "    VALUES ('{processo_nome}', 'inicio', 'executando', CURRENT_TIMESTAMP(), 'Iniciando testes de integridade e constantes');\n",
    "    \"\"\")\n",
    "\n",
    "    # --- TESTE 1: INTEGRIDADE REFERENCIAL (store_id) ---\n",
    "    print(\"Executando Teste de Integridade Referencial (store_id)...\")\n",
    "    \n",
    "    # Objetivo: Contar store_id na Fato (Gold) que NÃO existem na Dimensão (Gold)\n",
    "    df_ref_integrity = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) AS violation_count\n",
    "    FROM pl_delivery_analysis.tbl_fato_delivery_gold fact\n",
    "    LEFT ANTI JOIN pl_delivery_analysis.tbl_dim_store_gold dim\n",
    "        ON fact.store_id = dim.store_id\n",
    "    \"\"\")\n",
    "    violations_ref = df_ref_integrity.collect()[0]['violation_count']\n",
    "\n",
    "    # --- TESTE 2: VALIDAÇÃO DE CONSTANTES DA UE ---\n",
    "    print(\"Executando Teste de Validação de Constantes da UE...\")\n",
    "\n",
    "    # Objetivo: Recalcular a UE para uma amostra e comparar com o valor armazenado.\n",
    "    # Se houver diferença, as constantes ou a lógica de cálculo foram alteradas.\n",
    "    df_const_test = spark.sql(\"\"\"\n",
    "    WITH constantes AS (\n",
    "        SELECT \n",
    "            0.18 AS comissao_plataforma,\n",
    "            0.70 AS repasse_entregador,\n",
    "            0.02 AS taxa_transacao\n",
    "    ),\n",
    "    recalculo AS (\n",
    "        SELECT\n",
    "            p.order_id,\n",
    "            p.lucro_bruto_unitario AS stored_ue,\n",
    "            (\n",
    "                (p.subtotal_bruto * c.comissao_plataforma) + (p.delivery_fee_cliente * (1 - c.repasse_entregador)) \n",
    "                - (p.delivery_fee_cliente * c.repasse_entregador) \n",
    "                - (p.subtotal_bruto * c.taxa_transacao)\n",
    "            ) AS calculated_ue\n",
    "        FROM pl_delivery_analysis.tbl_fact_pedidos_silver p\n",
    "        CROSS JOIN constantes c\n",
    "        LIMIT 10000 -- Amostra rápida para verificar a lógica/constantes\n",
    "    )\n",
    "    SELECT COUNT(*) AS violation_count\n",
    "    FROM recalculo\n",
    "    WHERE ROUND(stored_ue, 4) != ROUND(calculated_ue, 4);\n",
    "    \"\"\")\n",
    "    violations_const = df_const_test.collect()[0]['violation_count']\n",
    "\n",
    "    # --- LOGICA DE VERIFICAÇÃO FINAL ---\n",
    "    if violations_ref > 0 or violations_const > 0:\n",
    "        status_final = 'alerta'\n",
    "        detalhes_log = f\"ALERTA DQ: Falhas de Integridade Referencial ({violations_ref}) ou Inconsistências de Constantes ({violations_const}).\"\n",
    "        print(f\"ALERTA DQ: {detalhes_log}\")\n",
    "    else:\n",
    "        detalhes_log = \"Testes de Integridade Referencial e Validação de Constantes passaram com sucesso.\"\n",
    "        \n",
    "    # --- REGISTRAR MÉTRICAS DE TESTE ---\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO pl_delivery_analysis.metricas_qualidade (metrica, valor, data_calculo)\n",
    "    VALUES \n",
    "    ('test_ref_integrity_violations', {violations_ref}, CURRENT_TIMESTAMP()),\n",
    "    ('test_const_validation_violations', {violations_const}, CURRENT_TIMESTAMP());\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    status_final = 'falha'\n",
    "    erro_msg = str(e).replace(\"'\", \"\") \n",
    "    detalhes_log = f\"ERRO CRÍTICO no Data Testing: {erro_msg}\"\n",
    "    print(f\"ERRO CRÍTICO: {detalhes_log}\")\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    # --- LOG FINAL ---\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO pl_delivery_analysis.log_processamento (processo, etapa, status, timestamp, detalhes)\n",
    "    VALUES ('{processo_nome}', 'fim', '{status_final}', CURRENT_TIMESTAMP(), '{detalhes_log}');\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c84637-2e1a-4b3d-8bd2-3d63d51a92a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "    metrica,\n",
    "    valor,\n",
    "    data_calculo\n",
    "FROM \n",
    "    pl_delivery_analysis.metricas_qualidade\n",
    "WHERE \n",
    "    metrica LIKE 'test_%'\n",
    "ORDER BY data_calculo DESC;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6981504100088509,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_Final_Quality_Checks.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
